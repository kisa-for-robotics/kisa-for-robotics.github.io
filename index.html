<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AlignDiff: Aligning Diverse Human Preferences via Behavior-customisable Diffusion Model">
  <!-- <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CoTDiffusion:Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts
</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<!--<body>-->

  <body>

  <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Generate Subgoal Images before Act: Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts
</h1>
          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">
              <a href="https://keunhong.com">Keunhong Park</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span>
          </div> -->
<!--
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://cotdiffusion.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robotics agents often struggle to understand and follow the multi-modal prompts in complex manipulation scenes which are challenging to be sufficiently and accurately described by text alone. Moreover, for long-horizon manipulation tasks, the deviation from general instruction tends to accumulate if lack of intermediate guidance from high-level subgoals. For this, we consider can we <i>generate subgoal images before act</i> to enhance the instruction following in long-horizon manipulation with multi-modal prompts?
Inspired by the great success of diffusion model in image generation tasks, we propose a novel hierarchical framework named as <b>CoTDiffusion</b> that incorporates diffusion model as a high-level planner to convert the general and multi-modal prompts into coherent visual subgoal plans, which further guide the low-level foundation model before action execution.
We design a semantic alignment module that can anchor the progress of generated keyframes along a coherent generation chain, unlocking the chain-of-thought reasoning ability of diffusion model. Additionally, we propose bi-directional generation and frame concat mechanism to further enhance the fidelity of generated subgoal images and the accuracy of instruction following.
The experiments cover various robotics manipulation scenarios including visual reasoning, visual rearrange, and visual constraints. CoTDiffusion achieves outstanding performance gain compared to the baselines without explicit subgoal generation, which proves that a subgoal image is worth a thousand words of instruction.
The details and visualizations are available at <a href="https://cotdiffusion.github.io/">https://cotdiffusion.github.io/</a>.
<!--              \url{https://cotdiffusion.github.io}.</a>.-->
          </p>
        </div>
      </div>
    </div>

         <!--/ Motivation Example. -->
    <hr style="border-top: 5px dotted #b8b8b8;">
    <!-- Method. -->
    <!-- <div class="columns">
      <div class="column is-10 is-offset-1">  -->
    <h2 class="title is-3">Motivation Example</h2>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
<!--      <img style="width: 100%" src="./static/images/AlignDiff_main.svg"/>-->
      <img style="width: 60%" src="./static/images/motivation.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>11111A motivation example of robotics manipulation tasks in multi-modal instructions.</strong>  The subgoal images are worth a thousand words, inspiring us to propose a novel framework CoTDiffusion to generate goal images step-by-step before act.
      </p>
    </div>
    <!--/ Abstract. -->
    <hr style="border-top: 5px dotted #b8b8b8;">
    <!-- Method. -->
    <!-- <div class="columns">
      <div class="column is-10 is-offset-1">  -->
    <h2 class="title is-3">Method</h2>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 100%" src="./static/images/method.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>Overview of CoTDiffusion.</strong> CoTDiffusion consists of a multi-modal encoder and vision encoder <em>V</em>, semantic alignment module <em>S</em>, conditional diffusion model <em>E</em>, and foundation model <em>F</em> for action planning.
The prompt and observation tokens are combined and fed into the semantic alignment module to identify the current reasoning chain step, providing progressive guidance for the diffusion model to generate the next subgoal image.
The generated keyframes are further fed to the foundation model which predicts action sequences to achieve the imagined goal scene and this recursive process repeats in a receding horizon control loop until the task is finished.
      </p>
    </div>




          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 90%" src="./static/images/coarse_to_fine.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>The two phases of coarse-to-fine alignment module .</strong> First, the aligned module is coarsely pre-trained to predict residual mask patches between subgoal images for aligning spatial semantics, focusing on salient differences rather than pixel details in textures or colors.
The semantic alignment module is then integrated into the diffusion model for step-wise image generation with fine-grained pixel reconstruction. Additionally, bi-directional generation and frame concatenation mechanism further enhance subgoal image fidelity and instruction following.      </p>
    </div>

                      <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 90%" src="./static/images/visualization_main.png"/>
                        </div>
          <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>The visualization of CoTDiffusion</strong> in three typical long-horizon tasks with multi-modal prompts in VIMA-BENCH.    </div>

      <!-- </div>
    </div> -->
    <!--/ Method. -->
    <hr style="border-top: 5px dotted #b8b8b8;">
    <h2 class="title is-3">Quantitative Results</h2>
    <!--/ Hopper. -->
      <h3 class="title is-4">Main Evaluation on Success Rate</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 60%" src="./static/images/table_1.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->
          The baselines can be divided into two kinds of planners, including <strong>abstract planner</strong> and <strong>visual planner</strong>.
<strong>Abstract planner</strong> like Gato, Flamingo and VIMA directly map general prompts to subsequent actions in an end-to-end manner. Gato and Flamingo gets low success rates on long-horizon tasks without explicit subgoal generation to correct the accumulative deviation errors from the instructions.
In contrast, <strong>visual planner</strong> like SuSIE and CoTDiffusion can generate intermediate goal images to guide the action planning, which can enhance instruction following for long-horizon tasks via visual planning.   </p>
    </div>

      <h3 class="title is-4">Robustness to Insufficient Perception</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 60%" src="./static/images/table_2.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->
          The experiments demonstrate that CoTDiffusion enjoys better robustness to restricted perception than abstract planners, highlighting the benefits of hierarchical framework decoupled visual planning and action planning. Accurate and grounded subgoal images generated in visual planners provide supplemental visual context, which can partly compensate for the insufficient perception to aid robustness under single-view.
The requirements for the low-level action planner are reduced to basic single-object manipulation primitives in short horizon by providing coherent subgoal images as visual landmarks, with less reliance on rich visual perceptions.   </div>


      <h3 class="title is-4">Fidelity of Image Generation</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 60%" src="./static/images/table_3.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->
          With step-wise sub-prompts decomposed in advance, the performance of SuSIE gets largely raised but still underperforms CoTDiffusion, which has no need to explicitly decompose the general prompts and can generate subgoal images in an implicit chain-of-thought manner. The coarse-to-fine semantic alignment training training allows developing spatial reasoning prior to synthesis.
Frame concatenation further guides coherent denoising by providing rich context information as visual priors to ground the current observation and enhance the fidelity of generation.  </div>


      <h3 class="title is-4">Accuracy of Instruction Following</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 60%" src="./static/images/table_4.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->
          We evaluate instruction following accuracy via CLIP similarity between generated keyframes and general prompts, normalized by the CLIP score between ground truth ultimate goal image and prompts. Without chain-of-thought reasoning abilities, SuSIE struggles to follow instructions when given general multi-modal prompts, let alone generate subgoal images with smooth progressions. The coarse alignment pretraining and bi-directional generation can assist CoTDiffusion in tracking the progress of generated keyframes throughout the entire chain and generate sequenced keyframes incrementally advancing prompt instructions.</div>


      <h3 class="title is-4">Generalization across Tasks</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 55%" src="./static/images/table_5.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->
          We evaluate the generalization ability in three levels with increasing difficulty: placement generalization which randomizes the novel placement of objects (L1), object generalization which provides the objects with novel attributes (L2), and  task combinatorial generalization which complexes the prompts with extra novel instruction (L3).
    When CoTDiffusion visualizes novel concepts into goal images, the foundation model can still accomplish the stack by simply achieving the provided subgoal, with no need to inherently understand novel skills like stack.
    </div>


      <h3 class="title is-4">Ablation Studies of Horizon Length</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 55%" src="./static/images/table_6.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->
          We conduct an additional experiment by increasing the number of objects and the corresponding manipulation steps to evaluate the success rate of different methods on more complex and longer-horizon manipulation tasks. The results demonstrate that CoTDiffusion enjoys better robustness for longer horizons compared to abstract planners, with the benefit from the explicit visual subgoals providing improved guidance for following complex instructions.    </div>



      <h3 class="title is-4">Ablation Studies of Model Capacity</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 66%" src="./static/images/table_7.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->
          We conduct additional ablation experiments on the model capacity of some key components, including the semantic alignment module, T5 tokenizer, and low-level foundation model. Based on the experiments, we chose T5-base, a 20M semantic alignment module, and a 40M foundation model as our final model configuration. </div>



 <hr style="border-top: 5px dotted #b8b8b8;">
    <h2 class="title is-3">Qualitative Examples</h2>
    <!--/ Hopper. -->
<!--      <h3 class="title is-4">Main Evaluation on Success Rate</h3>-->
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 66%" src="./static/images/table_8.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;  justify-content: center;">
      <p class="caption" style="text-align:center;"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->

<strong>Visualization of CoTDiffusion and VIMA in visual rearrange long-horizon tasks with multi-modal prompts.</strong>  </p>
    </div>


<!--      <h3 class="title is-4">Main Evaluation on Success Rate</h3>-->
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 66%" src="./static/images/table_9.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;  justify-content: center;">
      <p class="caption" style="text-align:center;"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->

<strong>Visualization of CoTDiffusion and VIMA in visual reasoning long-horizon tasks with multi-modal prompts.</strong>  </p>
    </div>

<!-- <hr style="border-top: 5px dotted #b8b8b8;">-->
<!--    <h2 class="title is-3">Qualitative Examples</h2>-->
<!--    &lt;!&ndash;/ Hopper. &ndash;&gt;-->
<!--      <h3 class="title is-4">Main Evaluation on Success Rate</h3>-->
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 66%" src="./static/images/table_10.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;  justify-content: center;">
      <p class="caption" style="text-align:center;"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->

<strong>Visualization of CoTDiffusion and VIMA in visual constraints long-horizon tasks with multi-modal prompts.</strong>  </p>
    </div>


          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 66%" src="./static/images/table_11.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;  justify-content: center;">
      <p class="caption" style="text-align:center;"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->

<strong>Visualization of CoTDiffusion and VIMA in novel generation tasks with novel concept with `stack'.</strong>  </p>
    </div>


          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 66%" src="./static/images/table_12.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;  justify-content: center;">
      <p class="caption" style="text-align:center;"><br />
<!--        <strong>The two phases of coarse-to-fine alignment module .</strong> -->

<strong>Visualization of CoTDiffusion and VIMA in novel generation tasks with novel concept with `rotate'.</strong>  </p>
    </div>


<hr style="border-top: 5px dotted #b8b8b8;">
    <!-- Method. -->
    <!-- <div class="columns">
      <div class="column is-10 is-offset-1">  -->
    <h2 class="title is-3">Pseudocodes of CoTDiffusion</h2>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
<!--      <img style="width: 100%" src="./static/images/AlignDiff_main.svg"/>-->
      <img style="width: 80%" src="./static/images/table_13.png"/>
    </div>
<!--    <div class="content has-text-justified" style="margin-top: 20px;">-->
<!--      <p class="caption"><br />-->
<!--        <strong>A motivation example of robotics manipulation tasks in multi-modal instructions.</strong>  The subgoal images are worth a thousand words, inspiring us to propose a novel framework CoTDiffusion to generate goal images step-by-step before act.-->
<!--      </p>-->
<!--    </div>-->

<hr style="border-top: 5px dotted #b8b8b8;">
    <!-- Method. -->
    <!-- <div class="columns">
      <div class="column is-10 is-offset-1">  -->
<!--    <h2 class="title is-3">Conclusion</h2>-->
<!--              <div class="content has-text-justified">-->
<!--          <p>-->
<!--            We presented \alg, a hierarchical framework that integrates diffusion model as high-level module to translate the general multi-modal prompts into coherent subgoal images, serves as the visual milestones to anchor the low-level foundation model to plan action sequences, termed as `generate subgoal images before act'. With the coarse-to-fine training for semantic alignment module, \alg can identify the progress of generated subgoals images along reasoning chains, unlocking the chain-of-thought reasoning capabilities of diffusion model for long-horizon manipulation tasks. The experiments cover various long-horizon manipulation scenarios in \vimabench, and \alg show the strong instruction following and outstanding performance gain compared to existed methods without visual planning.-->
<!--Incorporating commonsense knowledge from pre-trained MLLM like GPT-4V provides an avenue for more generalizable and promising reasoning in \alg, which leaves as our future work. .-->
<!--&lt;!&ndash;              \url{https://cotdiffusion.github.io}.</a>.&ndash;&gt;-->
<!--          </p>-->
<!--        </div>-->


    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            We presented <strong>CoTDiffusion</strong>, a hierarchical framework that integrates diffusion model as high-level module to translate the general multi-modal prompts into coherent subgoal images, serves as the visual milestones to anchor the low-level foundation model to plan action sequences, termed as <i>generate subgoal images before act</i>. With the coarse-to-fine training for semantic alignment module, CoTDiffusion can identify the progress of generated subgoals images along reasoning chains, unlocking the chain-of-thought reasoning capabilities of diffusion model for long-horizon manipulation tasks. The experiments cover various long-horizon manipulation scenarios in VIMA-BENCH, and CoTDiffusion shows the strong instruction following and outstanding performance gain compared to existed methods without visual planning.
Incorporating commonsense knowledge from pre-trained MLLM like GPT-4V provides an avenue for more generalizable and promising reasoning in CoTDiffusion, which leaves as our future work. .
<!--              \url{https://cotdiffusion.github.io}.</a>.-->
          </p>
        </div>
      </div>
    </div>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://cotdiffusion.github.io/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://cotdiffusion.github.io/">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



    </main>
  </body>
</html>
