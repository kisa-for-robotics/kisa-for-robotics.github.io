<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AlignDiff: Aligning Diverse Human Preferences via Behavior-customisable Diffusion Model">
  <!-- <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style>
      /* 使用CSS来控制视频的宽度 */
      video {
          width: 100%;
          height: auto; /* 让高度自适应，保持视频比例 */
      }
      select {
        padding: 8px;
        font-size: 16px;
        border: 1px solid #ccc;
        border-radius: 5px;
        appearance: none;
        background-color: #f8f8f8;
        cursor: pointer;
      }

      select:hover {
        background-color: #e0e0e0;
      }

      select:focus {
        outline: none;
        box-shadow: 0 0 5px rgba(0, 0, 0, 0.3);
      }
  </style>

  <title>kisa-for-robotics
</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<!--<body>-->

  <body>

  <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">KISA: A Unified Keyframe Identifier and Skill Annotator
            for Long-Horizon Robotics Demonstrations
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://kisa-for-robotics.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(Coming Soon)</span>
                  </a>
              </span>

</h1>
          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robotic manipulation tasks often span over long horizons and encapsulate multiple subtasks with different skills. Learning policies directly from long-horizon demonstrations is challenging without intermediate keyframes guidance and corresponding skill annotations.
            Existing approaches for keyframe identification often struggle to offer reliable decomposition for low accuracy and fail to provide semantic relevance between keyframes and skills. 
            For this, we propose a unified <b>K</b>eyframe <b>I</b>dentifier and <b>S</b>kill <b>A</b>notator（KISA） that utilizes pretrained visual-language representations for precise and interpretable decomposition of unlabeled demonstrations.
            Specifically, we develop a simple yet effective temporal enhancement module that enriches frame-level representations with expanded receptive fields to capture semantic dynamics at the video level.
            We further propose coarse contrastive learning and fine-grained monotonic encouragement to enhance the alignment between visual representations from keyframes and language representations from skills.
            The experimental results across three benchmarks demonstrate that KISA outperforms competitive baselines in terms of accuracy and interpretability of keyframe identification.
            Moreover, KISA exhibits robust generalization capabilities and the flexibility to incorporate various pretrained representations.
<!--              \url{https://cotdiffusion.github.io}.</a>.-->
          </p>
        </div>
      </div>
    </div>

         <!--/ Motivation Example. -->
    <hr style="border-top: 5px dotted #b8b8b8;">
    <!-- Method. -->
    <!-- <div class="columns">
      <div class="column is-10 is-offset-1">  -->
    <h2 class="title is-3">Motivation Example</h2>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
<!--      <img style="width: 100%" src="./static/images/AlignDiff_main.svg"/>-->
      <img style="width: 70%" src="./static/images/kisa_overview2_00.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>Overview of Keyframe Identification.</strong>  
        Both goal image similarity（LIV-I） and skill language similarity（LIV-L） struggle to stand out at keyframes. 
        KISA can exhibit conspicuous peaks near groundtruth boundaries for accurate keyframe identification.  
      </p>
    </div>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 70%" src="./static/images/motivation_00.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>Motivation Example.</strong> 
        Without historical context for semantic action cognition, two visually similar frames from different skills can confuse the alignment. Furthermore, the 
        representation might overfit the alignment between isolated frames and skills, leading to over-identification or false-identification.
      </p>
    </div>
    <!--/ Abstract. -->
    <hr style="border-top: 5px dotted #b8b8b8;">
    <!-- Method. -->
    <!-- <div class="columns">
      <div class="column is-10 is-offset-1">  -->
    <h2 class="title is-3">Method</h2>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 100%" src="./static/images/framework_v3_00.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>Overview framework of KISA.</strong>  
        KISA first leverages a simple yet effective temporal enhancement module upon the pre-trained vision-language representation to 
        obtain the video-level representation for each frame. During training, the alignment involves two branches: <b>Inter-skill</b>: we 
        design coarse history-aware contrastive learning via constructing hard negative samples with mismatched historical contexts and incorrect skills. 
        <b>Intra-skill</b> we additionally fine-grained monotonic alignment to encourage the capture of skill-aware progress within the sub-task, and prevent representation collapse to highly similarity within the same skill.
    </div>

      <!-- </div>
    </div> -->
    <!--/ Method. -->
    <hr style="border-top: 5px dotted #b8b8b8;">

    <h2 class="title is-3">Qualitative Results</h2>
    <!-- <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 70%" src="./static/images/kitchen0.png"/>
    </div> -->

    <video controls autoplay loop>
      <source src="videos/frankakitchen.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

    <h3 class="title is-4">Visualization of Keyframe Identification</h3>
    <div style="display: flex; justify-content: center; align-items: center;">
      <div style="width: 50%;">
        <img style="width: 100%;" src="./static/images/kitchen0.png" alt="Image 1"/>
      </div>
      <div style="width: 50%;">
        <img style="width: 100%;" src="./static/images/kitchen1.png" alt="Image 2"/>
      </div>
    </div>
    
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>The Visualizaton of KISA in FrankaKitchen: </strong>  
        (left)Move the kettle to the top-left burner, activate the bottom burner, turn on the light switch, and open the slide cabinet.
        (right)Open the Microwave Door, activate the bottom burner, activate the top burner and Open the left Hinge Cabinet.
    </div>

    <div style="display: flex; justify-content: center; align-items: center;">
      <div style="width: 50%;">
        <img style="width: 100%;" src="./static/images/calvin2.png" alt="Image 1"/>
      </div>
      <div style="width: 50%;">
        <img style="width: 100%;" src="./static/images/calvin4.png" alt="Image 2"/>
      </div>
    </div>
    
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>The Visualizaton of KISA in CALVIN: </strong>  
        (left)Rotate Block Right, Open Drawer, Turn on LED and Push into Drawer.
        (right)Rotate Block Right, Open Drawer, Push in Drawer, Slide Door Right and Lift Block from Drawer.
    </div>

    <div style="display: flex; justify-content: center; align-items: center;">
      <div style="width: 50%;">
        <img style="width: 100%;" src="./static/images/maniskill1.png" alt="Image 1"/>
      </div>
      <div style="width: 50%;">
        <img style="width: 100%;" src="./static/images/maniskill2.png" alt="Image 2"/>
      </div>
    </div>
    
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>The Visualizaton of KISA in Maniskill2: </strong>  
        (left)Pick up the peg, Move the Peg, and Insert the Peg into the Hole.
        (right)Pick up the Charger, Move the Charger, and Insert the Charger into Receptacle.
    </div>

    <h3 class="title is-4">Confidence Scores for Keyframe Identification</h3>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 80%" src="./static/images/confidence_calculation.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>A example of confidence score calculation.</strong> 
         For each step, the confidence score is the maximum similarity between video-level representation and language representation from skills.
      </p>
    </div>

    <h3 class="title is-4">The Heatmap of Alignment Score between Skills and Frames</h3>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 80%" src="./static/images/monotonic_align.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        The heatmap between skills and frames from a long-horizon demonstration example on CALVIN. The comparisons between
        LIV (a), KISA w/o monotonic alignment (b) and KISA (c).
      </p>
    </div>

    <h3 class="title is-4">Word Clouds</h3>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 80%" src="./static/images/wordcloud.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>World Clouds: </strong> 
        To visually summarize the key aspects covered by the diverse manipulation skills in ManiSkill2, CALVIN, and FrankaKitchen
        benchmarks, we created the word clouds. We now tokenize the instruction and for each skill code used in the
        trajectory, record all the tokens from the language instruction. Once we have this mapping from skills to tokens, we can plot
        heat maps and word clouds. The predominant terms reflect a heavy focus on interacting with household objects like boxes,
        cans, bottles, tools, and tableware. Terms such as ”pick up”, ”move”, ”rotate”, and ”stack” indicate the skills that aim to test
        fundamental robot capabilities in grasping, manipulating, and placing common items. The emergence of words depicting
        spatial reasoning like ”left/right” and goal configurations highlights skills requiring contextual understanding and mapping
        instructions to feasible actions. 
      </p>
    </div>

    <h3 class="title is-4">t-SNE Visualization</h3>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 70%" src="./static/images/tsne_calvin.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <strong>t-SNE Visualization of Skill Space in CALVIN </strong> 
        reveals the relationship between the defined skills based on their visual
        dynamics. Each point denotes one skill type, colored by category. The modular relationships indicate promise for the zero-shot
        composition of new behaviors by leveraging similarity within the embedding space during policy learning.
      </p>
    </div>


    <!-- <video controls>
      <source src="videos/blueberry.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video> -->


    <hr style="border-top: 5px dotted #b8b8b8;">

    <h2 class="title is-3">Quantitative Results</h2>

    <!--/ Hopper. -->
    <h3 class="title is-4">The evaluation results of keyframe identification</h3>
    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 100%" src="./static/images/table_1.jpg"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        We evaluate several baselines on the collected long-horizon demonstrations dataset with groundtruth skill labels three typical manipulation environments and report the mean and variance across 5 seeds. 
        KISA significantly outperforms all baselines across three metrics, highlighting its superior ability in keyframe identification. 
        Reward-driven keyframe extraction methods like VideoRLCS perform the worst among baselines. A potential reason is that the importance of the keyframe for reward prediction decreases when the horizon extends and the visual representation is not exploited. 
        On the other hand, unsupervised methods like KTS without additional training, which solely relies on the similarity of visual embeddings for clustering, can already achieve a higher accuracy than VideoRLCS. 
        This provides empirical insight that the visual information has great potential for keyframe identification. Moreover, we directly utilize pretrained robotics representations such as R3M, VIP, and LIV to assess the ability to identify keyframes. 
        The accuracy improves in the order of R3M, VIP, and LIV, with the latter enjoying more fine-grained representation properties. 
      </p>
    </div>

      <h3 class="title is-4">The comparisons of skill annotation</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 80%" src="./static/images/performance_00.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        On all three classic robotics representations, the skill annotation accuracy of the `KISA-' 
        version outperforms that of the `FT-' version, which follows the vanilla alignment technique with static representation. 
    </div>

      <h3 class="title is-4">Zero-shot Results</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 70%" src="./static/images/table_2.jpg"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        To systematically examine generalization capacities, we establish a 3-level protocol evaluating models on progressively more challenging unseen distributions without additional
        training: (L1) Object Generalization: We evaluate L1 genralization on rich manipulation scenes from Maniskill2, with diverse object colors, shapes, numbers, and placements.
        (L2) Combinatorial Generalization: We evaluate generalization on CALVIN for novel skill compositions that never appear in training and also examine the accuracy of decomposition in longer horizon cases. (L3) Embodiment Generalization.

      <h3 class="title is-4">Combinatorial Generalization (L2) on CALVIN</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 80%" src="./static/images/horizon_00.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        As the horizon extends, the combinatorial generalization ability of the KISA remains significantly stronger than baselines. 
        This is primarily due to KISA’s ability to integrate historical frames to expand receptive fields, which allows it to better capture long-range skill dynamics beyond isolated
        frames. Rather than overfitting to superficial environmental details, KISA focuses on learning on core semantics - reducing dependence on specific configurations or scenes.

    <h3 class="title is-4">Cross Embodiment Generalization (L3)</h3>
        <div style="display: flex; margin: auto; justify-content: center; width: 100%">
    <img style="width: 90%" src="./static/images/cross_embodiment4_00.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        The illustrating example of LIV and KISA for zero-shot generalization from simulators on long-horizon real robotics demonstration datasets.
    </div>

      <h3 class="title is-4">The Flexibility for Pre-trained Representations</h3>
          <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 60%" src="./static/images/table_3.jpg"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        KISA equips static frame-level representations with video-level understanding capability, which is flexible to incorporate with 
        any existing visual representation backbone. We conduct comprehensive evaluations across R3M, VIP, and LIV through ablations studies of the proposed temporal en-
        hancement module, history-aware contrastive learning, and monotonicity alignment components respectively.

    <div style="display: flex; margin: auto; justify-content: center; width: 100%">
      <img style="width: 90%" src="./static/images/mono_abla_main_text_v3_00.png"/>
    </div>
    <div class="content has-text-justified" style="margin-top: 20px;">
      <p class="caption"><br />
        <b>The heatmap between skills and frames from a long-horizon demonstration example on CALVIN.</b>
        The comparisons between KISA w/o monotonic alignment (left) and KISA (right).

    <h3 class="title is-4">The Effectiveness for Policy Learning</h3>
      <div style="display: flex; margin: auto; justify-content: center; width: 100%">
        <img style="width: 60%" src="./static/images/table_4.jpg"/>
      </div>
      <div class="content has-text-justified" style="margin-top: 20px;">
        <p class="caption"><br />
          Success rates on CALVIN including LCBC, LISA, and the variation with demonstration annotated by LIV and KISA. 
        Based on LISA, we compare when provided with privileged information including explicit keyframes and skill annotations to avoid re-discovering skills, while retaining
        low-level skill-conditioned policy learning for fair comparisons. Unsurprisingly, LISA+KISA achieves a significant performance improvement with annotated demonstrations, particularly in tasks with longer horizons.
    
    <hr style="border-top: 5px dotted #b8b8b8;">
    
    <h2 class="title is-3">Typical Cases</h2>

    <h3 class="title is-4">Success Case</h3>
    <div>
      <select onchange="changeVideo('success', this)">
        <option value="videos/frankakitchen.mp4">FrankaKitchen0</option>
        <option value="videos/blueberry.mp4">BrdigeData0_Blueberry</option>
      </select>

      <video id="successVideoPlayer" controls>
        <source id="successVideoSource" src="" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>

    <h3 class="title is-4">Failure Case</h3>
    <div>
      <select onchange="changeVideo('failure', this)">
        <option value="videos/calvin.mp4">CALVIN0</option>
        <option value="videos/cauliflowers.mp4">BrdigeData0_Cauliflowers</option>
      </select>

      <video id="failureVideoPlayer" controls>
        <source id="failureVideoSource" src="" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>

    <script>
      function changeVideo(type, selectElement) {
        var videoPlayerId = type === 'success' ? 'successVideoPlayer' : 'failureVideoPlayer';
        var videoSourceId = type === 'success' ? 'successVideoSource' : 'failureVideoSource';
        
        var videoPlayer = document.getElementById(videoPlayerId);
        var videoSource = document.getElementById(videoSourceId);

        // 获取所选视频的文件路径
        var selectedVideo = selectElement.value;

        // 更新视频的src属性
        videoSource.src = selectedVideo;

        // 重新加载视频
        videoPlayer.load();

        // 播放视频
        videoPlayer.play();
      }
    </script>


    <hr style="border-top: 5px dotted #b8b8b8;">

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            In this paper, we introduce KISA, a unified framework to achieve accurate keyframe identification and skills annotation for long-horizon manipulation demonstrations. 
            We propose a simple yet effective temporal enhanced module that can flexibly equip any existing pre-trained representations with expanded receptive fields to capture long-range
            semantic dynamics, bridging the gap between static frame-level representation and video-level understanding. We further design coarse contrastive learning and fine-grained
            monotonic encouragement to enhance the alignment between keyframes and skills. The experiment results demonstrate that KISA achieves more accurate and interpretable
            keyframe identification than competitive baselines and enjoys the robust zero-shot generalization ability. Furthermore, demonstrations with accurate keyframes and interpretable
            skills annotated by KISA can significantly facilitate policy learning. We believe that KISA can serve as a reliable tool to accurately annotate robotics demonstrations at a low cost,
            potentially facilitating the research in robotics community.
          </p>
        </div>
      </div>
    </div>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://cotdiffusion.github.io/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://cotdiffusion.github.io/">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



    </main>
  </body>
</html>
